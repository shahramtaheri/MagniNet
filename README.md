MagniNet

Magnification-Specific Hybrid CNNâ€“Transformer Framework for Breast Cancer Histopathology Classification

Overview

This repository provides the official PyTorch implementation of MagniNet, a magnification-specific hybrid CNNâ€“Transformer framework for breast cancer histopathology image classification. The model is designed to jointly perform binary benignâ€“malignant classification and eight-class histological subtype recognition, while maintaining strong interpretability and computational efficiency.

MagniNet integrates EfficientNet-based local feature extraction, cross-attention fusion, and a hierarchical Swin Transformer to capture both fine-grained cellular morphology and long-range contextual dependencies. Separate models are trained for each optical magnification (40Ã—, 100Ã—, 200Ã—, and 400Ã—), enabling scale-specific representation learning consistent with pathological diagnostic workflows.

The framework is evaluated on the BreakHis dataset and externally validated on BACH, Camelyon16, PatchCamelyon, and TCGA-BRCA, demonstrating robust generalization across staining protocols, acquisition devices, and magnification settings.

Key Contributions

Magnification-specific learning for multi-scale histopathology images

Hybrid CNNâ€“Transformer architecture combining EfficientNet and Swin Transformer

Cross-attention fusion of local morphological and global contextual features

Dual-head classification for binary and multi-class tasks

Integrated interpretability using SHAP, Grad-CAM++, and LIME

External validation on multiple independent histopathology datasets

Lightweight architecture with low parameter count and fast inference

Repository Structure
MagniNet/
â”œâ”€â”€ train_magninet.py
â”œâ”€â”€ test_magninet.py
â”œâ”€â”€ evaluate_external.py
â”œâ”€â”€ infer.py
â”œâ”€â”€ models/
â”œâ”€â”€ datasets/
â”œâ”€â”€ preprocessing/
â”œâ”€â”€ interpretability/
â”œâ”€â”€ analysis/
â”œâ”€â”€ configs/
â”œâ”€â”€ utils/
â”œâ”€â”€ notebooks/
â”œâ”€â”€ scripts/
â”œâ”€â”€ results/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ environment.yml
â”œâ”€â”€ CITATION.cff
â””â”€â”€ LICENSE


Each module is organized to clearly separate model definition, data handling, training, evaluation, interpretability, and analysis, ensuring full reproducibility.

Installation
Requirements

Python â‰¥ 3.9

PyTorch â‰¥ 2.0

timm

NumPy, SciPy, scikit-learn

matplotlib, seaborn

shap, lime

Install dependencies
pip install -r requirements.txt


or using Conda:

conda env create -f environment.yml
conda activate magninet

Datasets
Primary Dataset

BreakHis â€“ Breast Cancer Histopathological Database
https://web.inf.ufpr.br/vri/databases/breast-cancer-histopathological-database-breakhis/

External Validation Datasets

BACH (ICIAR 2018)

Camelyon16

PatchCamelyon

TCGA-BRCA

All datasets are publicly available and must be downloaded separately in accordance with their respective licenses.

Data Organization

For each magnification, datasets should follow the structure:

data/BreakHis/40x/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ images/
â”‚   â””â”€â”€ labels.json
â”œâ”€â”€ val/
â”‚   â”œâ”€â”€ images/
â”‚   â””â”€â”€ labels.json
â””â”€â”€ test/
    â”œâ”€â”€ images/
    â””â”€â”€ labels.json


Binary labels are either provided explicitly or derived from the eight-class annotations.

Training

MagniNet is trained independently for each magnification.

python train_magninet.py --config configs/magninet_40x.yaml


Training details:

Optimizer: AdamW

Learning rate: 1e-4

Scheduler: Cosine annealing

Mixed-precision training (AMP)

Loss:

ğ¿
=
ğ›¼
ğ¿
ğ‘š
ğ‘¢
ğ‘™
ğ‘¡
ğ‘–
+
(
1
âˆ’
ğ›¼
)
ğ¿
ğ‘
ğ‘–
ğ‘›
ğ‘
ğ‘Ÿ
ğ‘¦
,
ğ›¼
=
0.6
L=Î±L
multi
	â€‹

+(1âˆ’Î±)L
binary
	â€‹

,Î±=0.6
Evaluation
Internal Testing
python test_magninet.py --checkpoint best.pt

External Validation
python evaluate_external.py --checkpoint best.pt --dataset BACH


Evaluation metrics include accuracy, precision, recall, specificity, F1-score, and AUROC.

Interpretability

MagniNet integrates multiple interpretability techniques:

Grad-CAM++ for class-discriminative localization

SHAP for pixel-level attribution

LIME for superpixel-based explanations

Quantitative evaluation is performed using Pointing Game Accuracy and ROI Intersection-over-Union (IoU) against expert-annotated regions.

Computational Analysis

The repository includes scripts to compute:

Model parameters

FLOPs

Inference time

Statistical significance tests (McNemarâ€™s test)

These analyses correspond to the efficiency and statistical validation reported in the manuscript.

Reproducibility

All experiments are fully reproducible using the provided configuration files, scripts, and documented training protocols. Random seeds are fixed and dataset splits are explicitly defined.

Citation

If you use this code, please cite:

Shahram Taheri.
MagniNet: A Magnification-Specific Hybrid CNNâ€“Transformer Framework for Accurate Breast Cancer Histopathology Classification.
The Visual Computer, under review.


(Please update with the final DOI after publication.)

License

This repository is released for research and academic use. See the LICENSE file for details.

Contact

For questions or clarifications related to the implementation or experiments, please contact the corresponding author.
